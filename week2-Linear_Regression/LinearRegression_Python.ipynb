{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c979c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0308d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 0\n",
    "lr = 0.1 # learning_rate 学习率\n",
    "num = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650523c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -8.51333837 -11.24557416]\n",
      " [  3.10036361   5.55895282]\n",
      " [  6.34900913   9.90114356]\n",
      " [ -0.46229011   0.45043394]\n",
      " [ -2.24930507  -2.40848187]\n",
      " [  2.50878772   4.30242739]\n",
      " [ -4.16251076  -4.90433576]\n",
      " [  7.78304152  11.82059752]\n",
      " [  3.34712723   5.76299525]\n",
      " [  6.70032826  10.46374116]\n",
      " [  9.36256261  14.07626173]\n",
      " [  4.63004971   7.33672137]\n",
      " [ -5.72918202  -7.32983797]\n",
      " [  0.92952449   2.25435045]\n",
      " [ -0.25025503   0.60471922]\n",
      " [ -7.64573626  -9.81895783]\n",
      " [ -8.42820699 -11.16567992]\n",
      " [  9.50270259  14.2517835 ]\n",
      " [  4.71221465   7.57322925]\n",
      " [  4.98302859   8.02280275]\n",
      " [ -3.38927424  -4.04683762]\n",
      " [ -9.68910107 -12.81523269]\n",
      " [ -4.41497072  -5.45321542]\n",
      " [ -9.69396858 -12.67779489]\n",
      " [ -2.63589515  -2.97730074]\n",
      " [ -6.41423404  -8.07550434]\n",
      " [  7.23212265  11.10491725]\n",
      " [ -0.56594045   0.09796151]\n",
      " [  9.08782856  13.70431478]\n",
      " [  5.14644056   8.37632923]\n",
      " [ -7.00235181  -8.9564279 ]\n",
      " [ -8.14734388 -10.56348288]\n",
      " [ -1.32303887  -1.02503647]\n",
      " [ -6.3267825   -8.01217583]\n",
      " [ -6.80782194  -8.85133903]\n",
      " [  5.12005622   8.12669725]\n",
      " [  6.90523971  10.61904571]\n",
      " [ -7.61852471  -9.67591925]\n",
      " [  2.6666017    4.66473826]\n",
      " [  3.96176259   6.39531187]\n",
      " [  3.00536689   5.15042024]\n",
      " [  0.69903232   1.81281166]\n",
      " [  7.46042527  11.6225069 ]\n",
      " [ -0.45751779   0.1461517 ]\n",
      " [ -4.76046748  -5.94797381]\n",
      " [ -1.55291143  -1.3071722 ]\n",
      " [ -1.46847475  -1.12964428]\n",
      " [  4.83228313   7.7351898 ]\n",
      " [  4.1649491    6.84257637]\n",
      " [  5.44807323   8.6635495 ]\n",
      " [ -4.56373716  -5.46762793]\n",
      " [  7.29905584  11.055678  ]\n",
      " [ -4.35280731  -5.24612379]\n",
      " [  8.84025524  13.50912384]\n",
      " [  8.29614543  12.55239544]\n",
      " [ -8.9951615  -11.65641635]\n",
      " [ -2.99110925  -3.36072765]\n",
      " [  5.6912244    8.99604367]\n",
      " [ -3.34000361  -3.73054843]\n",
      " [  3.68325715   6.19313533]\n",
      " [ -9.35302045 -12.46531479]\n",
      " [ -3.44367325  -3.85393014]\n",
      " [ -7.56271122  -9.80099657]\n",
      " [  6.48024591   9.9878158 ]\n",
      " [  4.29406777   6.90010066]\n",
      " [ -9.58941497 -12.63799941]\n",
      " [  6.91693202  10.62462584]\n",
      " [  5.32239613   8.56223577]\n",
      " [ -5.74099643  -7.30242248]\n",
      " [ -0.838789    -0.37564469]\n",
      " [ -3.36493645  -3.77170028]\n",
      " [  8.80666268  13.35109852]\n",
      " [  3.08375941   5.17385536]\n",
      " [  5.27123012   8.49846317]\n",
      " [ -9.99138981 -13.42995896]\n",
      " [  3.51289077   5.78138784]\n",
      " [  1.47172791   2.96287484]\n",
      " [ -0.35549303   0.31218131]\n",
      " [  6.55296094  10.14989427]\n",
      " [ -3.27806857  -3.65998294]\n",
      " [ -2.57591392  -2.68401876]\n",
      " [  0.91302068   2.23202135]\n",
      " [  6.64184902  10.35378041]\n",
      " [  3.21139835   5.2508758 ]\n",
      " [  2.64085714   4.56727877]\n",
      " [  1.45499578   2.86772062]\n",
      " [  9.94040995  14.820152  ]\n",
      " [  7.36833051  11.2761881 ]\n",
      " [  8.886943    13.34769743]\n",
      " [  9.43317506  14.20162464]\n",
      " [ -4.88343253  -6.13164772]\n",
      " [ -4.19661746  -5.08806304]\n",
      " [  6.83386462  10.54396065]\n",
      " [  0.37435875   1.40881431]\n",
      " [  4.94832675   7.9412193 ]\n",
      " [  0.14789573   1.30852561]\n",
      " [  8.59360362  13.08265815]\n",
      " [  3.21595941   5.3326561 ]\n",
      " [ -6.30195677  -8.04539168]\n",
      " [  6.32746636   9.69327208]]\n"
     ]
    }
   ],
   "source": [
    "#加载数据\n",
    "data = []\n",
    "for i in range(100):\n",
    "    x = np.random.uniform(-10., 10.) # 加 .的意思是我们希望取出的不是整数，否则默认是整型\n",
    "    # 偏置量 normal通过正态分布来取偏置量  期望是0，方差是0.1\n",
    "    eps = np.random.normal(0.,0.1)\n",
    "    y = 1.41*x + 0.89 + eps\n",
    "    data.append([x,y])\n",
    "data = np.array(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c736f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "def loss():\n",
    "    loss = 0\n",
    "    for i in range(100):\n",
    "        x = data[i,0]\n",
    "        y = data[i,1]\n",
    "        \n",
    "        #计算损失函数求和wx+b-y\n",
    "        loss += pow((w*x+b-y),2)/100\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21cb2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义梯度\n",
    "def gradient():\n",
    "    w_gradient = 0\n",
    "    b_gradient = 0\n",
    "    for i in range(100):\n",
    "        x = data[i,0]\n",
    "        y = data[i,1]\n",
    "        #计算梯度\n",
    "        w_gradient += (2/100)*((w*x+b)-y)*x\n",
    "        b_gradient += (2/100)*((w*x+b)-y)\n",
    "    return w_gradient,b_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a526831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_gradient =  0.0008135396532900647 b_gradient =  -0.033265485691360924 loss =  0.010514156559005226\n",
      "w_gradient =  0.000797743644224063 b_gradient =  -0.03261958980876008 loss =  0.010503301501428659\n",
      "w_gradient =  0.0007822543367644527 b_gradient =  -0.031986234897152466 loss =  0.010492863883752219\n",
      "w_gradient =  0.0007670657759010283 b_gradient =  -0.031365177456063674 loss =  0.010482827652991524\n",
      "w_gradient =  0.0007521721221629973 b_gradient =  -0.030756178712923656 loss =  0.010473177373492501\n",
      "w_gradient =  0.0007375676494577356 b_gradient =  -0.03015900453126384 loss =  0.010463898203191364\n",
      "w_gradient =  0.0007232467430566339 b_gradient =  -0.029573425320701483 loss =  0.010454975870787848\n",
      "w_gradient =  0.000709203897030275 b_gradient =  -0.028999215948671154 loss =  0.010446396653795905\n",
      "w_gradient =  0.0006954337124562004 b_gradient =  -0.02843615565387623 loss =  0.010438147357438572\n",
      "w_gradient =  0.000681930895270761 b_gradient =  -0.02788402796139934 loss =  0.010430215294354643\n",
      "w_gradient =  0.0006686902540676172 b_gradient =  -0.027342620599496333 loss =  0.010422588265085433\n",
      "w_gradient =  0.000655706698525025 b_gradient =  -0.026811725417966347 loss =  0.010415254539312255\n",
      "w_gradient =  0.0006429752369361517 b_gradient =  -0.026291138308141396 loss =  0.010408202837815144\n",
      "w_gradient =  0.0006304909743636601 b_gradient =  -0.02578065912440791 loss =  0.010401422315125537\n",
      "w_gradient =  0.0006182491111164853 b_gradient =  -0.025280091607262245 loss =  0.010394902542845997\n",
      "w_gradient =  0.0006062449408069312 b_gradient =  -0.02478924330784378 loss =  0.010388633493611505\n",
      "w_gradient =  0.0005944738482537501 b_gradient =  -0.024307925513962454 loss =  0.010382605525667277\n",
      "w_gradient =  0.0005829313079649019 b_gradient =  -0.023835953177535243 loss =  0.010376809368039981\n",
      "w_gradient =  0.0005716128821297364 b_gradient =  -0.023373144843451748 loss =  0.010371236106278848\n",
      "w_gradient =  0.0005605142194057937 b_gradient =  -0.022919322579803156 loss =  0.010365877168745583\n",
      "w_gradient =  0.000549631052737632 b_gradient =  -0.022474311909472387 loss =  0.010360724313430962\n",
      "w_gradient =  0.0005389591978685083 b_gradient =  -0.022037941743069302 loss =  0.010355769615278806\n",
      "w_gradient =  0.0005284945521082142 b_gradient =  -0.0216100443131323 loss =  0.01035100545399739\n",
      "w_gradient =  0.0005182330919776858 b_gradient =  -0.021190455109649488 loss =  0.01034642450233929\n",
      "w_gradient =  0.0005081708723835496 b_gradient =  -0.020779012816796495 loss =  0.010342019714832259\n",
      "w_gradient =  0.0004983040248671237 b_gradient =  -0.02037555925091785 loss =  0.010337784316943321\n",
      "w_gradient =  0.0004886287560244493 b_gradient =  -0.01997993929971607 loss =  0.010333711794659542\n",
      "w_gradient =  0.00047914134602252204 b_gradient =  -0.019592000862620595 loss =  0.010329795884469594\n",
      "w_gradient =  0.0004698381473257525 b_gradient =  -0.019211594792302108 loss =  0.010326030563730411\n",
      "w_gradient =  0.00046071558320116834 b_gradient =  -0.018838574837336156 loss =  0.010322410041404566\n",
      "w_gradient =  0.000451770146407144 b_gradient =  -0.018472797585970948 loss =  0.010318928749153508\n",
      "w_gradient =  0.0004429983978566053 b_gradient =  -0.018114122410995605 loss =  0.010315581332773598\n",
      "w_gradient =  0.0004343969650388163 b_gradient =  -0.01776241141567813 loss =  0.01031236264396144\n",
      "w_gradient =  0.0004259625409866949 b_gradient =  -0.017417529380740666 loss =  0.01030926773239568\n",
      "w_gradient =  0.0004176918831541726 b_gradient =  -0.017079343712370576 loss =  0.01030629183812358\n",
      "w_gradient =  0.0004095818116705077 b_gradient =  -0.016747724391261706 loss =  0.010303430384240102\n",
      "w_gradient =  0.00040162920855835865 b_gradient =  -0.01642254392260319 loss =  0.010300678969848856\n",
      "w_gradient =  0.0003938310163081535 b_gradient =  -0.01610367728707852 loss =  0.010298033363293288\n",
      "w_gradient =  0.00038618423678649094 b_gradient =  -0.015791001892800104 loss =  0.010295489495648711\n",
      "w_gradient =  0.0003786859303066374 b_gradient =  -0.0154843975281617 loss =  0.01029304345446412\n",
      "w_gradient =  0.0003713332139080004 b_gradient =  -0.015183746315645699 loss =  0.010290691477744914\n",
      "w_gradient =  0.0003641232607290018 b_gradient =  -0.014888932666485047 loss =  0.010288429948167055\n",
      "w_gradient =  0.000357053298885808 b_gradient =  -0.014599843236229534 loss =  0.010286255387513554\n",
      "w_gradient =  0.00035012061024002755 b_gradient =  -0.014316366881174818 loss =  0.010284164451325035\n",
      "w_gradient =  0.0003433225293329213 b_gradient =  -0.014038394615623366 loss =  0.010282153923755973\n",
      "w_gradient =  0.00033665644283577614 b_gradient =  -0.013765819569981335 loss =  0.010280220712628726\n",
      "w_gradient =  0.00033011978761839783 b_gradient =  -0.013498536949688469 loss =  0.010278361844677857\n",
      "w_gradient =  0.00032371005068611985 b_gradient =  -0.013236443994910224 loss =  0.010276574460977187\n",
      "w_gradient =  0.000317424767776138 b_gradient =  -0.012979439941041847 loss =  0.010274855812542892\n",
      "w_gradient =  0.00031126152234551077 b_gradient =  -0.012727425979962063 loss =  0.010273203256105376\n",
      "w_gradient =  0.0003052179449821897 b_gradient =  -0.012480305222044422 loss =  0.010271614250044225\n",
      "w_gradient =  0.00029929171214304953 b_gradient =  -0.012237982658913004 loss =  0.010270086350478986\n",
      "w_gradient =  0.00029348054538588103 b_gradient =  -0.012000365126914447 loss =  0.010268617207510631\n",
      "w_gradient =  0.00028778221055224956 b_gradient =  -0.011767361271294605 loss =  0.010267204561607322\n",
      "w_gradient =  0.00028219451687097954 b_gradient =  -0.011538881511076272 loss =  0.010265846240129311\n",
      "w_gradient =  0.00027671531600674884 b_gradient =  -0.011314838004634663 loss =  0.010264540153987579\n",
      "w_gradient =  0.0002713425015629656 b_gradient =  -0.011095144615897279 loss =  0.01026328429443048\n",
      "w_gradient =  0.00026607400783449503 b_gradient =  -0.010879716881253573 loss =  0.01026207672995471\n",
      "w_gradient =  0.0002609078092434651 b_gradient =  -0.010668471977070618 loss =  0.010260915603334359\n",
      "w_gradient =  0.0002558419195501406 b_gradient =  -0.010461328687850548 loss =  0.01025979912876461\n",
      "w_gradient =  0.000250874391228062 b_gradient =  -0.010258207375005964 loss =  0.01025872558911526\n",
      "w_gradient =  0.00024600331437053466 b_gradient =  -0.010059029946248306 loss =  0.010257693333289684\n",
      "w_gradient =  0.00024122681635600773 b_gradient =  -0.009863719825555789 loss =  0.01025670077368551\n",
      "w_gradient =  0.00023654306057817667 b_gradient =  -0.00967220192374207 loss =  0.010255746383752843\n",
      "w_gradient =  0.00023195024659579747 b_gradient =  -0.009484402609576023 loss =  0.010254828695646537\n",
      "w_gradient =  0.00022744660852712026 b_gradient =  -0.009300249681482192 loss =  0.010253946297968573\n",
      "w_gradient =  0.00022303041478224928 b_gradient =  -0.009119672339786562 loss =  0.010253097833597352\n",
      "w_gradient =  0.00021869996771995207 b_gradient =  -0.008942601159476003 loss =  0.010252281997600515\n",
      "w_gradient =  0.00021445360235351069 b_gradient =  -0.008768968063531156 loss =  0.010251497535227857\n",
      "w_gradient =  0.00021028968595261655 b_gradient =  -0.008598706296742541 loss =  0.010250743239981666\n",
      "w_gradient =  0.0002062066178503269 b_gradient =  -0.008431750400043812 loss =  0.010250017951761009\n",
      "w_gradient =  0.00020220282826052816 b_gradient =  -0.00826803618534565 loss =  0.010249320555077579\n",
      "w_gradient =  0.0001982767778197607 b_gradient =  -0.008107500710865408 loss =  0.010248649977340008\n",
      "w_gradient =  0.00019442695717513184 b_gradient =  -0.007950082256917764 loss =  0.01024800518720443\n",
      "w_gradient =  0.00019065188617333959 b_gradient =  -0.007795720302195738 loss =  0.010247385192987964\n",
      "w_gradient =  0.00018695011345157275 b_gradient =  -0.007644355500494677 loss =  0.010246789041143775\n",
      "w_gradient =  0.00018332021577387864 b_gradient =  -0.007495929657905421 loss =  0.01024621581479441\n",
      "w_gradient =  0.0001797607976472583 b_gradient =  -0.007350385710426288 loss =  0.010245664632321664\n",
      "w_gradient =  0.0001762704906631725 b_gradient =  -0.0072076677020386825 loss =  0.010245134646010711\n",
      "w_gradient =  0.0001728479527710776 b_gradient =  -0.00706772076318637 loss =  0.010244625040746174\n",
      "w_gradient =  0.00016949186822985338 b_gradient =  -0.0069304910896851955 loss =  0.010244135032758726\n",
      "w_gradient =  0.00016620094680215927 b_gradient =  -0.006795925922029482 loss =  0.010243663868419387\n",
      "w_gradient =  0.0001629739231998953 b_gradient =  -0.006663973525113009 loss =  0.010243210823080676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_gradient =  0.00015980955683847498 b_gradient =  -0.006534583168342654 loss =  0.010242775199961909\n",
      "w_gradient =  0.00015670663101280402 b_gradient =  -0.006407705106133984 loss =  0.010242356329077745\n",
      "w_gradient =  0.00015366395282268713 b_gradient =  -0.006283290558774569 loss =  0.010241953566207674\n",
      "w_gradient =  0.00015068035243722203 b_gradient =  -0.006161291693682685 loss =  0.010241566291905108\n",
      "w_gradient =  0.00014775468285985194 b_gradient =  -0.006041661607010771 loss =  0.010241193910544844\n",
      "w_gradient =  0.000144885819321372 b_gradient =  -0.005924354305613573 loss =  0.010240835849406918\n",
      "w_gradient =  0.0001420726587243043 b_gradient =  -0.005809324689374304 loss =  0.01024049155779575\n",
      "w_gradient =  0.00013931411949700753 b_gradient =  -0.005696528533852864 loss =  0.010240160506193195\n",
      "w_gradient =  0.00013660914122686982 b_gradient =  -0.005585922473286618 loss =  0.010239842185444126\n",
      "w_gradient =  0.0001339566838802174 b_gradient =  -0.005477463983923852 loss =  0.010239536105973461\n",
      "w_gradient =  0.0001313557276639149 b_gradient =  -0.005371111367668165 loss =  0.010239241797033055\n",
      "w_gradient =  0.00012880527268708739 b_gradient =  -0.005266823736049857 loss =  0.010238958805977796\n",
      "w_gradient =  0.0001263043383654544 b_gradient =  -0.0051645609945080075 loss =  0.01023868669756934\n",
      "w_gradient =  0.00012385196318507458 b_gradient =  -0.005064283826973634 loss =  0.010238425053306846\n",
      "w_gradient =  0.0001214472042159083 b_gradient =  -0.004965953680752425 loss =  0.010238173470783232\n",
      "w_gradient =  0.0001190891370913584 b_gradient =  -0.0048695327517020055 loss =  0.010237931563066287\n",
      "w_gradient =  0.00011677685508363483 b_gradient =  -0.00477498396971062 loss =  0.010237698958103605\n",
      "w_gradient =  0.00011450946930925687 b_gradient =  -0.004682270984419441 loss =  0.010237475298150377\n",
      "w_gradient =  0.0001122861079517791 b_gradient =  -0.0045913581512780095 loss =  0.010237260239219084\n",
      "w_gradient =  0.00011010591632475489 b_gradient =  -0.0045022105178101415 loss =  0.010237053450550615\n",
      "w_gradient =  0.00010796805613727187 b_gradient =  -0.004414793810203772 loss =  0.010236854614105345\n",
      "w_gradient =  0.00010587170540140786 b_gradient =  -0.004329074420113111 loss =  0.010236663424074251\n",
      "w_gradient =  0.00010381605838984467 b_gradient =  -0.004245019391744572 loss =  0.010236479586408335\n",
      "w_gradient =  0.00010180032453969631 b_gradient =  -0.004162596409193831 loss =  0.010236302818366553\n",
      "w_gradient =  9.98237289719893e-05 b_gradient =  -0.004081773784009351 loss =  0.010236132848080861\n",
      "w_gradient =  9.788551183646105e-05 b_gradient =  -0.004002520443014211 loss =  0.01023596941413811\n",
      "w_gradient =  9.59849278640533e-05 b_gradient =  -0.003924805916366004 loss =  0.01023581226517805\n",
      "w_gradient =  9.412124636275568e-05 b_gradient =  -0.0038486003258371565 loss =  0.010235661159506618\n",
      "w_gradient =  9.229375091623568e-05 b_gradient =  -0.0037738743733177513 loss =  0.010235515864724341\n",
      "w_gradient =  9.05017387665269e-05 b_gradient =  -0.003700599329574698 loss =  0.01023537615736886\n",
      "w_gradient =  8.874452099676133e-05 b_gradient =  -0.0036287470231849937 loss =  0.010235241822571194\n",
      "w_gradient =  8.702142210323394e-05 b_gradient =  -0.0035582898297132778 loss =  0.010235112653725354\n",
      "w_gradient =  8.533177964071062e-05 b_gradient =  -0.0034892006610948356 loss =  0.010234988452170562\n",
      "w_gradient =  8.367494396679942e-05 b_gradient =  -0.0034214529552132333 loss =  0.01023486902688565\n",
      "w_gradient =  8.205027798917006e-05 b_gradient =  -0.0033550206657009998 loss =  0.010234754194195339\n",
      "w_gradient =  8.04571571895972e-05 b_gradient =  -0.0032898782519044053 loss =  0.010234643777487768\n",
      "w_gradient =  7.889496903134771e-05 b_gradient =  -0.0032260006690895036 loss =  0.010234537606942783\n",
      "w_gradient =  7.73631130371634e-05 b_gradient =  -0.003163363358791236 loss =  0.010234435519270833\n",
      "w_gradient =  7.586100006377536e-05 b_gradient =  -0.0031019422393900775 loss =  0.010234337357461726\n",
      "w_gradient =  7.438805276487048e-05 b_gradient =  -0.0030417136968368 loss =  0.01023424297054335\n",
      "w_gradient =  7.294370477061926e-05 b_gradient =  -0.0029826545755888565 loss =  0.010234152213349211\n",
      "w_gradient =  7.152740088265871e-05 b_gradient =  -0.0029247421696946404 loss =  0.010234064946295382\n",
      "w_gradient =  7.013859642622522e-05 b_gradient =  -0.0028679542140763715 loss =  0.010233981035165714\n",
      "w_gradient =  6.877675759161747e-05 b_gradient =  -0.0028122688759579587 loss =  0.010233900350905555\n",
      "w_gradient =  6.74413607332329e-05 b_gradient =  -0.0027576647464823863 loss =  0.010233822769422944\n",
      "w_gradient =  6.613189253021692e-05 b_gradient =  -0.002704120832470572 loss =  0.010233748171398173\n",
      "w_gradient =  6.484784945324715e-05 b_gradient =  -0.0026516165483627817 loss =  0.010233676442099967\n",
      "w_gradient =  6.358873777157631e-05 b_gradient =  -0.0026001317082891646 loss =  0.01023360747120914\n",
      "w_gradient =  6.235407366748241e-05 b_gradient =  -0.002549646518318136 loss =  0.010233541152648953\n",
      "w_gradient =  6.114338218662571e-05 b_gradient =  -0.0025001415688476916 loss =  0.010233477384421905\n",
      "w_gradient =  5.9956197965643776e-05 b_gradient =  -0.0024515978271395437 loss =  0.010233416068452907\n",
      "w_gradient =  5.879206463811973e-05 b_gradient =  -0.00240399662999857 loss =  0.01023335711043845\n",
      "w_gradient =  5.765053456207453e-05 b_gradient =  -0.0023573196766090813 loss =  0.010233300419701448\n",
      "w_gradient =  5.6531168923037306e-05 b_gradient =  -0.0023115490214844826 loss =  0.010233245909051975\n",
      "w_gradient =  5.5433537255348414e-05 b_gradient =  -0.0022666670675814803 loss =  0.010233193494653027\n",
      "w_gradient =  5.435721765402815e-05 b_gradient =  -0.0022226565595213714 loss =  0.01023314309589161\n",
      "w_gradient =  5.3301796363938725e-05 b_gradient =  -0.0021795005769638075 loss =  0.010233094635254774\n",
      "w_gradient =  5.2266867491806285e-05 b_gradient =  -0.0021371825281059262 loss =  0.010233048038210403\n",
      "w_gradient =  5.1252033311740414e-05 b_gradient =  -0.0020956861432889743 loss =  0.010233003233092543\n",
      "w_gradient =  5.025690345215883e-05 b_gradient =  -0.002054995468764095 loss =  0.01023296015099123\n",
      "w_gradient =  4.928109541657219e-05 b_gradient =  -0.0020150948605400223 loss =  0.010232918725646516\n",
      "w_gradient =  4.83242340994644e-05 b_gradient =  -0.001975968978368463 loss =  0.010232878893346495\n",
      "w_gradient =  4.738595157510392e-05 b_gradient =  -0.0019376027798695102 loss =  0.01023284059282933\n",
      "w_gradient =  4.6465887094337965e-05 b_gradient =  -0.0018999815147167356 loss =  0.010232803765189117\n",
      "w_gradient =  4.556368704698374e-05 b_gradient =  -0.00186309071898908 loss =  0.0102327683537852\n",
      "w_gradient =  4.4679004410359824e-05 b_gradient =  -0.0018269162096030463 loss =  0.010232734304155066\n",
      "w_gradient =  4.38114990644993e-05 b_gradient =  -0.0017914440788618369 loss =  0.01023270156393062\n",
      "w_gradient =  4.296083755449265e-05 b_gradient =  -0.0017566606890988493 loss =  0.010232670082757635\n",
      "w_gradient =  4.212669284488213e-05 b_gradient =  -0.0017225526674466713 loss =  0.010232639812218275\n",
      "w_gradient =  4.130874430941997e-05 b_gradient =  -0.001689106900688055 loss =  0.010232610705756677\n",
      "w_gradient =  4.0506677333653623e-05 b_gradient =  -0.0016563105302237905 loss =  0.010232582718607254\n",
      "w_gradient =  3.972018361768359e-05 b_gradient =  -0.0016241509471152113 loss =  0.010232555807726001\n",
      "w_gradient =  3.8948960682298056e-05 b_gradient =  -0.0015926157872463032 loss =  0.010232529931724165\n",
      "w_gradient =  3.819271220411105e-05 b_gradient =  -0.0015616929265639635 loss =  0.010232505050804636\n",
      "w_gradient =  3.7451147382096955e-05 b_gradient =  -0.0015313704764257849 loss =  0.010232481126700685\n",
      "w_gradient =  3.6723981082509294e-05 b_gradient =  -0.001501636779024285 loss =  0.010232458122617284\n",
      "w_gradient =  3.601093367494937e-05 b_gradient =  -0.0014724804029000523 loss =  0.010232436003174205\n",
      "w_gradient =  3.5311731069548324e-05 b_gradient =  -0.0014438901385560284 loss =  0.010232414734352014\n",
      "w_gradient =  3.462610441065314e-05 b_gradient =  -0.0014158549941410453 loss =  0.01023239428343934\n",
      "w_gradient =  3.395379021985459e-05 b_gradient =  -0.0013883641912238653 loss =  0.010232374618982845\n",
      "w_gradient =  3.329452997212837e-05 b_gradient =  -0.0013614071606539335 loss =  0.01023235571073867\n",
      "w_gradient =  3.264807018089899e-05 b_gradient =  -0.0013349735384983654 loss =  0.010232337529626136\n",
      "w_gradient =  3.201416226722431e-05 b_gradient =  -0.0013090531620491086 loss =  0.010232320047682784\n",
      "w_gradient =  3.1392562526572076e-05 b_gradient =  -0.0012836360659245955 loss =  0.01023230323802153\n",
      "w_gradient =  3.078303210136274e-05 b_gradient =  -0.0012587124782317814 loss =  0.010232287074789192\n",
      "w_gradient =  3.018533654617836e-05 b_gradient =  -0.0012342728168176012 loss =  0.010232271533126844\n",
      "w_gradient =  2.959924604584857e-05 b_gradient =  -0.001210307685577657 loss =  0.010232256589131525\n",
      "w_gradient =  2.9024535275583296e-05 b_gradient =  -0.0011868078708435724 loss =  0.010232242219819485\n",
      "w_gradient =  2.846098340322442e-05 b_gradient =  -0.0011637643378453804 loss =  0.010232228403090805\n",
      "w_gradient =  2.7908373689048555e-05 b_gradient =  -0.0011411682272377491 loss =  0.010232215117695436\n",
      "w_gradient =  2.7366493599394887e-05 b_gradient =  -0.0011190108516876632 loss =  0.010232202343200575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_gradient =  2.68351348729004e-05 b_gradient =  -0.0010972836925416227 loss =  0.010232190059959071\n",
      "w_gradient =  2.6314093231498414e-05 b_gradient =  -0.001075978396546768 loss =  0.010232178249079444\n",
      "w_gradient =  2.5803168339017662e-05 b_gradient =  -0.0010550867726418184 loss =  0.01023216689239659\n",
      "w_gradient =  2.530216377981745e-05 b_gradient =  -0.0010346007888033465 loss =  0.010232155972444057\n",
      "w_gradient =  2.481088692187286e-05 b_gradient =  -0.0010145125689636821 loss =  0.010232145472427014\n",
      "w_gradient =  2.432914894244867e-05 b_gradient =  -0.000994814389976768 loss =  0.01023213537619648\n",
      "w_gradient =  2.3856764477969694e-05 b_gradient =  -0.0009754986786610687 loss =  0.010232125668224576\n",
      "w_gradient =  2.3393552089055053e-05 b_gradient =  -0.0009565580088662113 loss =  0.010232116333580468\n",
      "w_gradient =  2.2939333538932183e-05 b_gradient =  -0.0009379850986413296 loss =  0.01023210735790754\n",
      "w_gradient =  2.2493934355400508e-05 b_gradient =  -0.0009197728074152819 loss =  0.010232098727401301\n",
      "w_gradient =  2.205718322462885e-05 b_gradient =  -0.0009019141332685283 loss =  0.010232090428788032\n",
      "w_gradient =  2.1628912155700897e-05 b_gradient =  -0.0008844022102362219 loss =  0.01023208244930458\n",
      "w_gradient =  2.120895665914252e-05 b_gradient =  -0.0008672303056512722 loss =  0.010232074776678497\n",
      "w_gradient =  2.0797155085780505e-05 b_gradient =  -0.0008503918175906478 loss =  0.010232067399109406\n",
      "w_gradient =  2.0393349295096813e-05 b_gradient =  -0.0008338802722981388 loss =  0.010232060305250628\n",
      "w_gradient =  1.9997383928188617e-05 b_gradient =  -0.0008176893217301758 loss =  0.010232053484191843\n",
      "w_gradient =  1.9609106766741574e-05 b_gradient =  -0.0008018127410897328 loss =  0.010232046925442289\n",
      "w_gradient =  1.922836854561724e-05 b_gradient =  -0.0007862444264441094 loss =  0.010232040618914693\n",
      "w_gradient =  1.8855022868476123e-05 b_gradient =  -0.0007709783923801152 loss =  0.010232034554909652\n",
      "w_gradient =  1.848892624418605e-05 b_gradient =  -0.0007560087696957488 loss =  0.01023202872410076\n",
      "w_gradient =  1.8129938000922136e-05 b_gradient =  -0.0007413298031491039 loss =  0.010232023117520292\n",
      "w_gradient =  1.7777919871137665e-05 b_gradient =  -0.0007269358492503074 loss =  0.010232017726545327\n",
      "w_gradient =  1.743273670751222e-05 b_gradient =  -0.0007128213740762407 loss =  0.010232012542884623\n",
      "w_gradient =  1.7094255787723084e-05 b_gradient =  -0.0006989809511578921 loss =  0.01023200755856572\n",
      "w_gradient =  1.676234691132608e-05 b_gradient =  -0.0006854092593886951 loss =  0.010232002765922777\n",
      "w_gradient =  1.6436882482508824e-05 b_gradient =  -0.0006721010809786006 loss =  0.010231998157584736\n",
      "w =  1.4136748967813795 b =  0.8911783478861143\n"
     ]
    }
   ],
   "source": [
    "for j in range(num):\n",
    "    w_gradient = 0\n",
    "    b_gradient = 0\n",
    "    w_gradient,b_gradient = gradient()\n",
    "    #梯度不断更新\n",
    "    w = w - (lr * w_gradient)\n",
    "    b = b - (lr * b_gradient)\n",
    "    if j%100 == 0:\n",
    "        print('w_gradient = ',w_gradient,'b_gradient = ',b_gradient, 'loss = ',loss())\n",
    "print('w = ',w,'b = ',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b0e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
